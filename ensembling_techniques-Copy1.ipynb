{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice assignment: Advanced ensembling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XgQKq6Ldl-g"
   },
   "source": [
    "In this programming assignment, you are going to work with a dataset based on the following data:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
    "\n",
    "_Citation:_\n",
    "\n",
    "* _K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal._\n",
    "\n",
    "The dataset contains the information about the internet news articles. In this assignment, you are going to predict a number of shares of the news article (target column: `shares`). The information about the features is available through the link above. You are going to construct several machine learning algorithms (XGBoost, LightGBM, CatBoost and Lasso) and blend them into the final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qx7Y3W-_3LPT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ROnt-v3A5Rzy"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.917808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.407072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572592</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.496875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.396465</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.850694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.352778</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.583732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.804762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1578.0</td>\n",
       "      <td>0.429864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624595</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.593790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.332037</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>10.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.942282</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.360069</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>11.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795082</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.702439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>12.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652422</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.636029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.376667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>13.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740113</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.273927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.433333</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>9.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.578275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748538</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.504702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>-0.112500</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0                 6.0              73.0         0.916667               1.0   \n",
       "1                10.0            1280.0         0.407072               1.0   \n",
       "2                 9.0             576.0         0.535088               1.0   \n",
       "3                 9.0             210.0         0.583732               1.0   \n",
       "4                13.0            1578.0         0.429864               1.0   \n",
       "...               ...               ...              ...               ...   \n",
       "39639            10.0             745.0         0.487500               1.0   \n",
       "39640            11.0             205.0         0.671642               1.0   \n",
       "39641            12.0             544.0         0.551102               1.0   \n",
       "39642            13.0             303.0         0.574468               1.0   \n",
       "39643             9.0             319.0         0.578275               1.0   \n",
       "\n",
       "       n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  \\\n",
       "0                      0.980392        6.0             0.0       1.0   \n",
       "1                      0.572592       53.0             2.0      10.0   \n",
       "2                      0.693182        9.0             5.0       1.0   \n",
       "3                      0.729927        6.0             2.0       1.0   \n",
       "4                      0.624595       34.0            24.0      11.0   \n",
       "...                         ...        ...             ...       ...   \n",
       "39639                  0.587629       29.0             3.0       1.0   \n",
       "39640                  0.795082        5.0             1.0       0.0   \n",
       "39641                  0.652422       27.0            10.0      12.0   \n",
       "39642                  0.740113        3.0             3.0       3.0   \n",
       "39643                  0.748538       10.0             6.0       0.0   \n",
       "\n",
       "       num_videos  average_token_length  ...  min_positive_polarity  \\\n",
       "0             0.0              4.917808  ...               0.150000   \n",
       "1             1.0              4.496875  ...               0.033333   \n",
       "2             1.0              4.850694  ...               0.100000   \n",
       "3             0.0              4.804762  ...               0.062500   \n",
       "4             0.0              4.593790  ...               0.033333   \n",
       "...           ...                   ...  ...                    ...   \n",
       "39639        21.0              4.942282  ...               0.100000   \n",
       "39640         1.0              4.702439  ...               0.100000   \n",
       "39641         1.0              4.636029  ...               0.033333   \n",
       "39642         0.0              4.273927  ...               0.100000   \n",
       "39643         0.0              4.504702  ...               0.100000   \n",
       "\n",
       "       max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
       "0                   0.500000              -0.200000                 -0.200   \n",
       "1                   1.000000              -0.396465                 -1.000   \n",
       "2                   0.900000              -0.352778                 -1.000   \n",
       "3                   0.500000              -0.250000                 -0.400   \n",
       "4                   1.000000              -0.332037                 -1.000   \n",
       "...                      ...                    ...                    ...   \n",
       "39639               0.800000              -0.360069                 -0.800   \n",
       "39640               0.433333              -0.466667                 -0.800   \n",
       "39641               0.600000              -0.376667                 -0.800   \n",
       "39642               1.000000              -0.433333                 -0.500   \n",
       "39643               0.900000              -0.112500                 -0.125   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "0                  -0.200000            0.000000                  0.000000   \n",
       "1                  -0.050000            0.000000                  0.000000   \n",
       "2                  -0.100000            0.000000                  0.000000   \n",
       "3                  -0.100000            0.454545                  0.136364   \n",
       "4                  -0.071429            0.000000                  0.000000   \n",
       "...                      ...                 ...                       ...   \n",
       "39639              -0.066667            0.900000                 -0.500000   \n",
       "39640              -0.200000            0.600000                  0.400000   \n",
       "39641              -0.050000            0.666667                 -0.125000   \n",
       "39642              -0.300000            0.000000                  0.000000   \n",
       "39643              -0.100000            0.200000                 -0.100000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity  shares  \n",
       "0                    0.500000                      0.000000     878  \n",
       "1                    0.500000                      0.000000    1100  \n",
       "2                    0.500000                      0.000000     872  \n",
       "3                    0.045455                      0.136364    5000  \n",
       "4                    0.500000                      0.000000    1600  \n",
       "...                       ...                           ...     ...  \n",
       "39639                0.400000                      0.500000    5800  \n",
       "39640                0.100000                      0.400000    1100  \n",
       "39641                0.166667                      0.125000     845  \n",
       "39642                0.500000                      0.000000    1300  \n",
       "39643                0.300000                      0.100000    1800  \n",
       "\n",
       "[39644 rows x 59 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-95PM9m56JZ"
   },
   "source": [
    "## 1\n",
    "\n",
    "**q1:** How many missing values are there in the data? Provide the number of cells in the dataframe that contain NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "self_reference_max_shares\n",
       "0.00         0.0\n",
       "1.59         0.0\n",
       "4.00         0.5\n",
       "9.00         0.0\n",
       "24.00        0.0\n",
       "            ... \n",
       "652900.00    0.0\n",
       "663600.00    0.0\n",
       "690400.00    0.0\n",
       "837700.00    0.0\n",
       "843300.00    0.0\n",
       "Name: weekday_is_monday, Length: 1137, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['self_reference_max_shares']).median()['weekday_is_monday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "AuyhBV3ULyIj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_tokens_title                   0\n",
       "n_tokens_content                 0\n",
       "n_unique_tokens                  0\n",
       "n_non_stop_words                 0\n",
       "n_non_stop_unique_tokens         0\n",
       "num_hrefs                        0\n",
       "num_self_hrefs                   0\n",
       "num_imgs                         0\n",
       "num_videos                       0\n",
       "average_token_length             0\n",
       "num_keywords                     0\n",
       "data_channel_is_lifestyle        0\n",
       "data_channel_is_entertainment    0\n",
       "data_channel_is_bus              0\n",
       "data_channel_is_socmed           0\n",
       "data_channel_is_tech             0\n",
       "data_channel_is_world            0\n",
       "kw_min_min                       0\n",
       "kw_max_min                       0\n",
       "kw_avg_min                       0\n",
       "kw_min_max                       0\n",
       "kw_max_max                       0\n",
       "kw_avg_max                       0\n",
       "kw_min_avg                       0\n",
       "kw_max_avg                       0\n",
       "kw_avg_avg                       0\n",
       "self_reference_min_shares        0\n",
       "self_reference_max_shares        0\n",
       "self_reference_avg_sharess       0\n",
       "weekday_is_monday                0\n",
       "weekday_is_tuesday               0\n",
       "weekday_is_wednesday             0\n",
       "weekday_is_thursday              0\n",
       "weekday_is_friday                0\n",
       "weekday_is_saturday              0\n",
       "weekday_is_sunday                0\n",
       "is_weekend                       0\n",
       "LDA_00                           0\n",
       "LDA_01                           0\n",
       "LDA_02                           0\n",
       "LDA_03                           0\n",
       "LDA_04                           0\n",
       "global_subjectivity              0\n",
       "global_sentiment_polarity        0\n",
       "global_rate_positive_words       0\n",
       "global_rate_negative_words       0\n",
       "rate_positive_words              0\n",
       "rate_negative_words              0\n",
       "avg_positive_polarity            0\n",
       "min_positive_polarity            0\n",
       "max_positive_polarity            0\n",
       "avg_negative_polarity            0\n",
       "min_negative_polarity            0\n",
       "max_negative_polarity            0\n",
       "title_subjectivity               0\n",
       "title_sentiment_polarity         0\n",
       "abs_title_subjectivity           0\n",
       "abs_title_sentiment_polarity     0\n",
       "shares                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-PSn5w66Ylo"
   },
   "source": [
    "## 2\n",
    "\n",
    "**q2:** What is the maximum number of shares among all the news articles presented in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tA6IUIgZL0JP"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# df.shares.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7SEP8pj9VQf"
   },
   "source": [
    "## 3\n",
    "\n",
    "**q3:** What is the median number of shares for the articles published on Monday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"shares\"][df[\"weekday_is_monday\"]==1.0].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJRiUrSt9gWS"
   },
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36BGDOx19peU"
   },
   "source": [
    "First, we separate the target from the dataframe with features (`df` -> `X`, `y`).\n",
    "\n",
    "Next, let's split the data into train/val/test sets in the ratio 60:20:20. The idea is that we will use train set to train our models, val set to validate them and test set to calculate the final error of the blend. So, test set will be a completely unseen data.\n",
    "\n",
    "To do this, use a regular `train_test_split` from `sklearn` to split `X` and `y` into train and val/test parts in the ratio 60:40. Then use `train_test_split` again, but to split the obtain val/test part into validation and test in the ratio 50:50. In each `train_test_split` application, use `random_state=13` and other default parameter values.\n",
    "\n",
    "In the end, you should obtain `X_train`, `X_val`, `X_test` with the following shapes, respectively: (23786, 58), (7929, 58), (7929, 58). The same logic is with `y_train`, `y_val`, `y_test`.\n",
    "\n",
    "**q4:** What is the mean value of target in the test part (`X_test`)? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "q5: Calculate Root Mean Squared Error (RMSE) on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "q6: Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "q7: Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "x18Nt2Ln9hAQ"
   },
   "outputs": [],
   "source": [
    "X = df.drop('shares', axis=1)\n",
    "y = df['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.40, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n_tokens_title                       10.410203\n",
       "n_tokens_content                    545.915878\n",
       "n_unique_tokens                       0.529930\n",
       "n_non_stop_words                      0.969038\n",
       "n_non_stop_unique_tokens              0.671886\n",
       "num_hrefs                            10.970803\n",
       "num_self_hrefs                        3.331378\n",
       "num_imgs                              4.635641\n",
       "num_videos                            1.239501\n",
       "average_token_length                  4.543182\n",
       "num_keywords                          7.248833\n",
       "data_channel_is_lifestyle             0.053727\n",
       "data_channel_is_entertainment         0.178837\n",
       "data_channel_is_bus                   0.154685\n",
       "data_channel_is_socmed                0.061168\n",
       "data_channel_is_tech                  0.180477\n",
       "data_channel_is_world                 0.212259\n",
       "kw_min_min                           25.819208\n",
       "kw_max_min                         1148.876059\n",
       "kw_avg_min                          313.698171\n",
       "kw_min_max                        13415.726826\n",
       "kw_max_max                       752747.786606\n",
       "kw_avg_max                       258897.450445\n",
       "kw_min_avg                         1113.592503\n",
       "kw_max_avg                         5656.212980\n",
       "kw_avg_avg                         3138.715397\n",
       "self_reference_min_shares          3809.919521\n",
       "self_reference_max_shares          9930.941139\n",
       "self_reference_avg_sharess         6185.947327\n",
       "weekday_is_monday                     0.166351\n",
       "weekday_is_tuesday                    0.186657\n",
       "weekday_is_wednesday                  0.191323\n",
       "weekday_is_thursday                   0.180161\n",
       "weekday_is_friday                     0.143398\n",
       "weekday_is_saturday                   0.063186\n",
       "weekday_is_sunday                     0.068924\n",
       "is_weekend                            0.132110\n",
       "LDA_00                                0.183948\n",
       "LDA_01                                0.141448\n",
       "LDA_02                                0.216224\n",
       "LDA_03                                0.226463\n",
       "LDA_04                                0.231916\n",
       "global_subjectivity                   0.442882\n",
       "global_sentiment_polarity             0.118833\n",
       "global_rate_positive_words            0.039554\n",
       "global_rate_negative_words            0.016647\n",
       "rate_positive_words                   0.680974\n",
       "rate_negative_words                   0.287874\n",
       "avg_positive_polarity                 0.353292\n",
       "min_positive_polarity                 0.095411\n",
       "max_positive_polarity                 0.756124\n",
       "avg_negative_polarity                -0.260142\n",
       "min_negative_polarity                -0.525009\n",
       "max_negative_polarity                -0.107411\n",
       "title_subjectivity                    0.283938\n",
       "title_sentiment_polarity              0.070916\n",
       "abs_title_subjectivity                0.340935\n",
       "abs_title_sentiment_polarity          0.158996\n",
       "dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.25, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_validate_test_split(df, train_percent, validate_percent, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    validate = df.iloc[perm[train_end:validate_end]]\n",
    "    test = df.iloc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3639.594577553594"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"shares\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3639.594577553594"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['shares'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = train_validate_test_split(df, train_percent=.5, validate_percent=.25, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3508.833114721017"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['shares'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JBpG87eiL7DY"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zijl0TGp_8P2"
   },
   "source": [
    "## 5\n",
    "\n",
    "Now let's train our first model - XGBoost. A link to the documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "We will use Scikit-Learn Wrapper interface for XGBoost (and the same logic applies to the following LightGBM and CatBoost models). Here, we work on the regression task - hence we will use `XGBRegressor`. Read about the parameters of `XGBRegressor`: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "\n",
    "The main list of XGBoost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html# Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `XGBRegressor` with MSE objective (`objective='reg:squarederror'`), 200 trees (`n_estimators=200`), `learning_rate=0.01`, `max_depth=5`, `random_state=13` and all other default parameter values. Train it on the train set (`fit` function). \n",
    "\n",
    "**q5:** Calculate Root Mean Squared Error (RMSE) on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e8a4e48d302e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxgb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracyxgboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgb0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredict_xgboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmsexgboost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_xgboost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrmsexgboost\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsexgboost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb1' is not defined"
     ]
    }
   ],
   "source": [
    "xgb1.fit(X_train,y_train)\n",
    "accuracyxgboost = round(xgb0.score(X_train, y_train)*100,2)\n",
    "predict_xgboost = xgb0.predict(x_test)\n",
    "msexgboost = mean_squared_error(y_test,predict_xgboost)\n",
    "rmsexgboost= np.sqrt(msexgboost)\n",
    "rmsexgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bsf1mDiPL-dX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
       "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.01, max_delta_step=None, max_depth=5,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='reg:squarederror', random_state=13, reg_alpha=None,\n",
       "              reg_lambda=None, scale_pos_weight=None, subsample=None,\n",
       "              tree_method=None, validate_parameters=None, verbosity=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "# predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "xgb1 = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    random_state=13,\n",
    "    objective='reg:squarederror')\n",
    "xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1.fit(validate, test['shares'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9YFI8rhBTmN"
   },
   "source": [
    "## 6\n",
    "\n",
    "In the task 5, we have decided to build 200 trees in our model. However, it is hard to understand whether it is a good decision - maybe it is too much? Maybe 150 is a better number? Or 100? Or 50 is enough?\n",
    "\n",
    "During the training process, it is possible to stop constructing the ensemble if we see that the validation error does not decrease anymore. Using the same XGBoost model, call `fit` function (to train it) with `eval_set=[(X_val, y_val)]` (to evaluate the boosting model after building a new tree) and `early_stopping_rounds=50` (and other default parameter values). This `early_stopping_rounds` says that if the validation metric does not increase on 50 consequent iterations, the training stops.\n",
    "\n",
    "**q6:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmX6FqvoMA4L"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_jy-BqrCaci"
   },
   "source": [
    "## 7\n",
    "\n",
    "Notes on parameter tuning: https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `XGBRegressor` with the following parameters:\n",
    "\n",
    "* `objective='reg:squarederror'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=4`\n",
    "* `gamma=1`\n",
    "* `subsample=0.5`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 6, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q7:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4nDYySWMC_p"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57TAxOUfC_-9"
   },
   "source": [
    "## 8\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 7. \n",
    "\n",
    "**q8:** What is the name of the most important feature? Provide it as the answer. Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that by default, `XGBRegressor` calculates feature importance considering gain (`importance_type` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyWWYYvKMEwO"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4EWKwRkEJGO"
   },
   "source": [
    "## 9\n",
    "\n",
    "Let's move to LightGBM. We will work with `LGBMRegressor`.\n",
    "\n",
    "LGBMRegressor parameters: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor\n",
    "\n",
    "The main list of LightGBM parameters: https://lightgbm.readthedocs.io/en/latest/Parameters.html Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `LGBMRegressor` with the following parameters, similar to the previous `XGBoost` model:\n",
    "\n",
    "* `objective='regression'`\n",
    "* `n_estimators=200`\n",
    "* `learning_rate=0.01`\n",
    "* `max_depth=5`\n",
    "* `random_state=13`\n",
    "* other default parameter values\n",
    "\n",
    "Train it on the training data with `eval_set=[(X_val, y_val)]`, `eval_metric='rmse'`, `early_stopping_rounds=50` and all other default parameter values. \n",
    "\n",
    "**q9:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm and compare it to the speed of XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Y-dWz4SMHTB"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9CrcUB3GZg9"
   },
   "source": [
    "## 10\n",
    "\n",
    "Notes on parameter tuning: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `LGBMRegressor` with the following parameters:\n",
    "\n",
    "* `objective='regression'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=3`\n",
    "* `lambda_l2=1.0`\n",
    "* `boosting_type='goss'`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 9, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q10:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybE86ky2MJLx"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRMPMVMqJEJu"
   },
   "source": [
    "## 11\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 10. \n",
    "\n",
    "**q11:** What is the name of the most important feature? Provide it as the answer. \n",
    "\n",
    "Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that by default, `LGBMRegressor` calculates feature importance considering number of times the feature is used in the model (`importance_type` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYrvkH1rMK6t"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjzix3YhSy5x"
   },
   "source": [
    "## 12\n",
    "\n",
    "Since some features are not important for the model, we can drop them in order to try to construct a better model which does not consider them at all.\n",
    "\n",
    "Obtain new train and validation sets without the features with LightGBM importance less than 10 (the importances were computed in the task 11). Train the same model as in the task 10 on the new train set in the same manner. \n",
    "\n",
    "**q12:** Calculate RMSE on the new validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice that the new versions of train and validation sets are used only in this task and in blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FEgaXIXMNkh"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pIu6_n3JvGI"
   },
   "source": [
    "## 13\n",
    "\n",
    "Let's move to CatBoost. We will work with `CatBoostRegressor`.\n",
    "\n",
    "Info about `CatBoostRegressor`: https://catboost.ai/docs/concepts/python-reference_catboostregressor.html\n",
    "\n",
    "CatBoost parameters: https://catboost.ai/docs/concepts/python-reference_parameters-list.html Look through this list so that you understand which parameters are presented in the library.\n",
    "\n",
    "Take `CatBoostRegressor` with the following parameters, similar to the previous models:\n",
    "\n",
    "* `loss_function='RMSE'`\n",
    "* `iterations=200`\n",
    "* `learning_rate=0.01`\n",
    "* `max_depth=5`\n",
    "* `random_state=13`\n",
    "* other default parameter values\n",
    "\n",
    "Train it on the training data with `eval_set=[(X_val, y_val)]`, `early_stopping_rounds=50` and all other default parameter values. \n",
    "\n",
    "**q13:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Notice the speed of the algorithm and compare it to the speed of XGBoost and LightGBM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-rFegFdMPx6"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keSiZN2DNYbn"
   },
   "source": [
    "## 14\n",
    "\n",
    "Notes on parameter tuning: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
    "\n",
    "Here, we tuned some parameters of the algorithm. Take `CatBoostRegressor` with the following parameters:\n",
    "\n",
    "* `loss_function='RMSE'`\n",
    "* `n_estimators=5000`\n",
    "* `learning_rate=0.001`\n",
    "* `max_depth=9`\n",
    "* `random_state=13`\n",
    "* all other default parameter values\n",
    "\n",
    "Train it in the same manner, as in the task 13, but with `early_stopping_rounds=500`. \n",
    "\n",
    "**q14:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-1QF1nKMRye"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d0kLl7HS3lG"
   },
   "source": [
    "## 15\n",
    "\n",
    "Calculate feature importances according to the model, trained in the task 14. \n",
    "\n",
    "**q15:** What is the name of the most important feature? Provide it as the answer. \n",
    "\n",
    "Do you understand why it might be important for the model?\n",
    "\n",
    "Notice that in case of regression, `CatBoostRegressor` calculates feature importance considering PredictionValuesChange: https://catboost.ai/docs/concepts/fstr.html#fstr__regular-feature-importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0-Bw-IbMVW3"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMdjrX7TdSx"
   },
   "source": [
    "## 16\n",
    "\n",
    "Finally, take a `Lasso` model from `sklearn` with `alpha=10.0`, `random_state=13` and all other default parameter values. Train it on the train set. \n",
    "\n",
    "**q16:** Calculate RMSE on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-3Ie1OmMXbj"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6COwSmbBt1m8"
   },
   "source": [
    "## 17\n",
    "\n",
    "Compare the results on the validation set of the trained models:\n",
    "\n",
    "* XGBoost (task 7)\n",
    "* LightGBM (task 12)\n",
    "* CatBoost (task 14)\n",
    "* Lasso (task 16)\n",
    "\n",
    "**q17:** Which model has the best RMSE value on the validation set? For the answer, provide the following:\n",
    "\n",
    "* 1 (if XGBoost was the best)\n",
    "* 2 (if LightGBM was the best)\n",
    "* 3 (if CatBoost was the best)\n",
    "* 4 (if Lasso was the best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1aEMdDB0cY01"
   },
   "source": [
    "## 18\n",
    "\n",
    "Finally, let's move to blending the models that we obtained. First, calculate the predictions for the trained models on the validation set. Remember that LightGBM model used slightly different set of columns in the data.\n",
    "\n",
    "After getting the predictions for the validation set, concatenate them into a single dataframe `X_val_blend`. The dataframe should look like this:\n",
    "\n",
    "||xgb|lgb|cb|lasso|\n",
    "|-|-|-|-|-|\n",
    "|0|2298.947754|3728.088336|3680.924182|4270.039931|\n",
    "|1|3208.189209|5243.744431|4487.549790|6755.853939|\n",
    "|...|...|...|...|...|\n",
    "\n",
    "Here, `xgb` column represents XGBoost predictions, `lgb` - LightGBM predictions, `cb` - CatBoost predictions, `lasso` - lasso predictions.\n",
    "\n",
    "**q18:** For the answer, calculate the mean value of all model predictions in the last row of this column (`X_val_blend.iloc[-1]`). Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLLOMwNSUG9k"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RikEkjthVKX-"
   },
   "source": [
    "## 19\n",
    "\n",
    "Obtain a matrix of pairwise Pearson Correlation Coefficient (PCC) values for the column of the dataframe `X_val_blend`. Find a pair of model predictions with the highest PCC value (don't consider 1.0 values of correlations with themselves). \n",
    "\n",
    "**q19:** What is this value equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7wJfNuTMh4A"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKrXVxTGVaZa"
   },
   "source": [
    "## 20\n",
    "\n",
    "Blend models into the ensemble with the weights 0.25, 0.25, 0.25 and 0.25 (just mean value of the predictions). \n",
    "\n",
    "**q20:** Calculate RMSE of such ensemble on the validation set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Compare it with RMSE of each model and think whether this is a good ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoD2EsNRMjvQ"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPbgfyadERE"
   },
   "source": [
    "## 21\n",
    "\n",
    "Tune the weights of the ensemble. Run each model weight through `np.linspace(0, 1, 101)`, so that all possible values of each weight will be [0.0, 0.01, 0.02, ..., 0.99, 1.0]. Skip each combinations of weights, if their sum is not equal to 1.0. If the sum of the weights in the combination is equal to 1.0, though, get ensemble prediction on the validation set using these weights and calculate RMSE value.\n",
    "\n",
    "In the end, select a combination of weights with the best RMSE value - these are the best weights for the ensemble. \n",
    "\n",
    "**q21:** What is their corresponding RMSE value equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568).\n",
    "\n",
    "Compare RMSE value of the ensemble with RMSE values of the models in it. Is the ensemble better?\n",
    "\n",
    "_Hint. You probably want to save RMSE with the corresponding weights for each valid combination into some array. Also this weight tuning might be implemented as quadriple nested loop, or you may think about other ways of implementing it. You can track tuning progress using `tqdm` module._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9Tud8e-Mp0C"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFqDAt-EdKxS"
   },
   "source": [
    "## 22\n",
    "\n",
    "Using the best weights obtained in the task 21, run the best ensemble on the test set. To do this, obtain model predictions on the test set (you can write them to the similar table to the one for the validation set in the task 18). Remember that LightGBM model uses slightly different set of columns.\n",
    "\n",
    "**q22:** Calculate RMSE of the final ensemble on the test set. What is it equal to? Provide the answer, rounded to the nearest FIVE decimal places (e.g. 12.3456789 -> 12.34568)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dEW1M-s6MtOx"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8xzctGVVD8ZayBsxVM3cd",
   "collapsed_sections": [],
   "name": "Week2_practice.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "ensembling-techniques-task-week-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
